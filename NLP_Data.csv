Q,A
What is linear regression?,"Linear regression is a statistical method used to model and analyze the relationships between a dependent variable and one or more independent variables. The main type of linear regression is simple linear regression, which is used when there are two variables."
What are the types of regression?,"There are multiple types of regression, including: 1) Linear Regression 2) Multiple Regression 3) Polynomial Regression 4) Ridge Regression 5) Lasso Regression 6) Logistic Regression, among others."
What is logistic regression?,Logistic regression is used when the dependent variable is categorical in nature. It estimates the probability that a given instance belongs to a particular category.
What is statistical significance?,"Statistical significance indicates whether an observed result in a study is likely to be genuine or if it could be due to random chance. Typically, a result is considered statistically significant if its p-value is below a predetermined threshold, commonly 0.05."
How can you evaluate regression models?,"Regression models can be evaluated using metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared, and Adjusted R-squared."
What are evaluation metrics?,"Evaluation metrics are a set of measures used to assess the performance of a machine learning model or algorithm. They help in understanding how well a model is performing in terms of accuracy, precision, recall, etc., relative to the problem's requirements."
What are the different types of evaluation metrics?,"The types of evaluation metrics depend on the type of machine learning problem: For regression tasks, commonly used metrics are Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared. For classification tasks, commonly used metrics include accuracy, precision, recall, F1-score, and Area Under the ROC Curve (AUC-ROC)."
What is the difference between training and testing data and what is it used for?,"Training data is used to train a machine learning model, allowing it to learn patterns or relationships within the data. Testing data, which the model hasn't seen during training, is used to evaluate the model's performance to ensure it generalizes well to new, unseen data."
What is cross validation and what are the benefits of it?,"Cross-validation is a technique where the training dataset is split multiple times into different training and validation sets. It helps in assessing the model's performance more robustly, reducing the risk of overfitting, and making better use of limited data. Common methods include k-fold cross-validation."
What is overfitting?,"Overfitting occurs when a model learns the training data too closely, including its noise and outliers. This results in a model that performs well on the training data but poorly on new, unseen data."
What is underfitting?,"Underfitting happens when a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and testing datasets."
How do you fix overfitting?,Some common methods to combat overfitting include: 1) Gathering more data 2) Using regularization techniques 3) Reducing the complexity of the model 4) Using cross-validation 5) Pruning (for decision trees) 6) Early stopping (for neural networks).
How do you fix underfitting?,"To address underfitting, one can: 1) Increase model complexity 2) Gather more relevant features 3) Reduce regularization 4) Modify model architecture (for neural networks)."
How do you handle imbalanced data in a dataset?,"Handling imbalanced data can involve: 1) Resampling techniques (either oversampling the minority class or undersampling the majority class) 2) Using synthetic data generation methods like SMOTE 3) Using different evaluation metrics that are more sensitive to imbalances, such as the F1-score or AUC-ROC 4) Adjusting class weights in the algorithm."
What should you do if your data is skewed?,"For skewed data, one can: 1) Apply log or square root transformations 2) Use non-linear algorithms that can handle skewness 3) Bin the data into different categories 4) Use algorithms robust to outliers like tree-based methods."
How can you identify outliers in your dataset and how can you fix it?,"Outliers can be identified using methods like visual inspection (e.g., scatter plots, box plots), statistical measures (e.g., z-scores, IQR). To fix outliers, one can: 1) Remove them 2) Cap them at certain threshold values 3) Use robust algorithms that are less sensitive to outliers 4) Transform the data."
What is the difference between extrapolation and interpolation?,"Interpolation involves estimating values between two known values in a dataset, while extrapolation involves predicting values outside the range of the known data points. The former is generally more reliable than the latter, which can have higher uncertainty."
What is supervised learning?,"Supervised learning is a type of machine learning where a model is trained on labeled data, meaning both the input and the corresponding desired output are provided. The model learns a mapping from inputs to outputs. Common examples include classification and regression tasks."
What is unsupervised learning?,"Unsupervised learning deals with unlabeled data, aiming to find patterns or structures in the data without explicit guidance. Common methods include clustering (grouping similar data points) and dimensionality reduction."
What is reinforcement learning?,Reinforcement learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward. The agent receives feedback in the form of rewards or penalties and adjusts its strategy accordingly.
What is data science?,"Data science is an interdisciplinary field that uses scientific methods, algorithms, and systems to extract insights and knowledge from structured and unstructured data. It encompasses various techniques from statistics, machine learning, and big data analytics to analyze and interpret complex data."
What is machine learning?,Machine learning is a subset of artificial intelligence that allows computers to learn from and make decisions based on data without being explicitly programmed. It involves algorithms that improve their performance or make accurate predictions based on past data.
What is natural language processing?,"Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and humans through natural language. It aims to make it possible for computers to read, understand, and derive meaning from human language."
What is the difference between data science and data analytics?,"Data science encompasses a broad set of tools and techniques to handle, analyze, and visualize data, including machine learning and advanced analytics. Data analytics, on the other hand, is more focused on processing and performing statistical analysis of datasets to discover insights or inform business decisions."
Why is Python used in data science?,"Python is popular in data science due to its simplicity and readability, combined with a wide range of data analytics libraries like Pandas, NumPy, and Scikit-learn. Its versatility and open-source nature, along with a strong supporting community, make it an ideal choice for various data-related tasks."
What is logistic regression?,Logistic regression is a statistical method used for modeling the probability of a binary outcome based on one or more predictor variables. It's commonly used for classification tasks such as spam detection or medical diagnosis.
What is mean squared error and what is it used for?,Mean Squared Error (MSE) is a metric that measures the average squared difference between the predicted values and the actual values. It is used to evaluate the performance of regression models; lower values indicate a better fit of the model to the data.
What is gradient descent and how do you solve it?,"Gradient descent is an optimization algorithm used to minimize a function iteratively. It's commonly used in training machine learning models. By calculating the gradient of the loss function with respect to the model's parameters, the model updates its parameters in the direction that decreases the error. The process is repeated until convergence or a set number of iterations."
In what cases is linear regression used for?,"Linear regression is used when there's a linear relationship between the independent and dependent variables. It's commonly applied for predicting numerical values, such as stock prices, sales forecasting, and trend analysis."
In what cases is logistic regression used for?,"Logistic regression is used for binary classification problems, where the outcome can take one of two classes. Examples include spam detection, customer churn prediction, and medical diagnosis (e.g., disease presence or absence)."
What is confusion matrix?,"A confusion matrix is a table used to evaluate the performance of classification models by comparing the actual vs. predicted classifications. It contains values like True Positives, False Positives, True Negatives, and False Negatives."
What is the true positive rate and the false positive rate?,"The True Positive Rate (TPR), also known as sensitivity or recall, measures the proportion of actual positives that are correctly identified. The False Positive Rate (FPR) measures the proportion of actual negatives that are incorrectly identified as positive."
How is data science different from software engineering?,"Data science focuses on analyzing and interpreting complex data to derive insights and build predictive models, using statistical, mathematical, and computational methods. Software engineering, on the other hand, is about designing, developing, and maintaining software applications, ensuring they run efficiently and meet user requirements."
What are the differences between supervised and unsupervised learning?,"In supervised learning, models are trained on labeled data, meaning both inputs and desired outputs are provided. It's used for tasks like classification and regression. Unsupervised learning deals with unlabeled data and aims to find patterns or structures, with methods like clustering or dimensionality reduction."
What is long format data and wide format data?,"Long format data (or tidy format) has one row for each observation and one column for each variable, making it long and narrow. Wide format data has one row per subject and variables in both rows and columns, making it wide. For instance, a dataset with daily temperature readings would have a single column for temperature in long format and separate columns for each day in wide format."
What is structured data and unstructured data?,"Structured data is organized in a predefined manner, often in tables with rows and columns, like databases or Excel spreadsheets. Unstructured data doesn't have a specific form or structure, including texts, images, videos, and more."
What is sampling and what is it used for?,"Sampling involves selecting a subset of individuals from a larger population to estimate characteristics of the whole population. It's used when analyzing the entire dataset is impractical or time-consuming. Methods include random sampling, stratified sampling, and cluster sampling."
Mention some techniques used for sampling?,Common sampling techniques include: 1) Simple Random Sampling 2) Stratified Sampling 3) Cluster Sampling 4) Systematic Sampling 5) Convenience Sampling.
What is sampling bias?,"Sampling bias occurs when some members of the intended population are more or less likely to be included in the sample than others, leading to a non-representative sample. This can skew results and reduce the generalizability of the findings."
What is dimensionality reduction?,"Dimensionality reduction refers to techniques used to reduce the number of input variables in a dataset. It can help in visualization, improving model performance, and reducing computational costs. Principal Component Analysis (PCA) and t-SNE are common methods."
What is classification?,"Classification is a type of supervised learning where the goal is to predict the categorical class labels of new instances, based on past observations. Examples include email spam detection and image categorization."
What are decision trees?,"Decision trees are a type of machine learning model that make decisions based on posing a series of questions to the data, each question narrowing the possible values, much like the game ""20 Questions."" They are intuitive and can be visualized graphically."
What is data cleaning and how can you use Python to clean data?,"Data cleaning involves identifying and correcting errors and inconsistencies in data to enhance its quality. With Python, libraries like Pandas can be used for tasks such as handling missing values, removing duplicates, and converting data types."
What is data wrangling?,"Data wrangling, often termed as data munging, is the process of transforming and mapping data from its raw form into another format for better understanding and use in further analysis."
What is data munging?,"Data munging is synonymous with data wrangling. It involves cleaning, structuring and enriching raw data into a desired format for better decision-making in less time."
What is data analytics?,"Data analytics involves examining raw data to draw conclusions and extract insights. It encompasses a variety of quantitative and qualitative techniques, from basic statistics to complex machine learning models, to understand and derive insights from data."
What is data mining?,"Data mining is the process of discovering patterns and knowledge from large amounts of data. The sources can include databases, data warehouses, and more. It involves methods at the intersection of machine learning, statistics, and database systems."
What is data warehousing?,"A data warehouse is a large, centralized repository of data that combines data from various sources to support business intelligence activities, including data analytics and reporting. It separates analytical processing from transactional databases to ensure data consistency and performance."
What are some Python data science libraries and what are they used for?,Some popular Python data science libraries include: 1) Pandas: data manipulation and analysis 2) NumPy: numerical operations and matrix computation 3) Scikit-learn: machine learning algorithms 4) Matplotlib and Seaborn: data visualization 5) TensorFlow and PyTorch: deep learning.
What is a loss function?,"A loss function, in machine learning, measures how well a prediction model does in terms of being able to predict the expected outcome. It quantifies the difference between the predicted and actual values."
What is a sigmoid function?,The sigmoid function is an S-shaped curve that can take any real-valued number and map it between 0 and 1. It's often used in logistic regression to represent probabilities.
What is the difference between a cost function and a loss function?,"The loss function computes the error for a single training example, while the cost function is the average of the loss functions for all the training examples. In simpler terms, loss function is applied to individual data points, and the cost function is the overall performance metric of the model on the entire dataset."
What is k-fold cross validation?,"K-fold cross validation is a technique where the training dataset is split into 'k' equal-sized subsets. The model is trained on k-1 of these folds and tested on the remaining fold. This process is repeated k times, each time with a different test fold. It helps in assessing the model's generalization capability."
Explain how a recommender system works.,Recommender systems suggest items to users based on learned patterns from past user interactions. Techniques include collaborative filtering (based on users' past behavior) and content-based filtering (based on item attributes). Hybrid methods combine both approaches.
What is Poisson distribution?,The Poisson distribution represents the number of events occurring in a fixed interval of time or space. It's characterized by the average rate (lambda) of the events. It's often used in scenarios like counting the number of emails arriving in an hour or the number of phone calls at a call center.
What is a Naive Bayes classifier and what is it used for?,"Naive Bayes is a probabilistic classifier based on Bayes' theorem, with the ""naive"" assumption of independence between every pair of features. It's often used for text classification tasks, like spam detection and sentiment analysis."
What is Gaussian distribution?,"The Gaussian distribution, also known as the normal distribution, is a bell-shaped curve characterized by a mean (mu) and standard deviation (sigma). Data values in a Gaussian distribution are symmetrically distributed around the mean."
What is uniform distribution?,"In a uniform distribution, all values have the same frequency/probability. For a continuous uniform distribution, any value within a range has an equal likelihood of occurring."
What is normal distribution?,"The normal distribution is another term for the Gaussian distribution. It describes a continuous variable whose probabilities are symmetrically distributed around the mean, forming a bell-shaped curve."
What is standard deviation?,"Standard deviation measures the amount of variation or dispersion of a set of values from the mean. A low standard deviation indicates values are close to the mean, while a high one suggests a wider spread."
What is deep learning?,"Deep learning is a subset of machine learning that utilizes neural networks with many layers (hence ""deep""). It's particularly effective for large and complex datasets, such as images and natural language processing tasks."
What are neural networks?,"Neural networks are computational models inspired by the human brain's network of neurons. They consist of layers of interconnected nodes or ""neurons"" that process information and can adapt to perform specific tasks by adjusting their weights based on input data."
What is deep learning used for?,"Deep learning has various applications, including image and speech recognition, language translation, playing games, medical diagnosis, and financial forecasting, among others."
What is a convolutional Neural Network?,Convolutional Neural Networks (CNNs) are a type of deep learning model primarily used for image processing and computer vision tasks. They use convolutional layers to scan for local features in an image.
What is a recurrent neural network?,"Recurrent Neural Networks (RNNs) are designed for sequential data and tasks where past information is crucial for future predictions, like time series forecasting or natural language processing. They have connections that loop back on themselves, allowing them to retain past information."
Explain selection bias.,Selection bias occurs when the sample obtained is not representative of the population intended to be analyzed. This can result in incorrect conclusions as certain segments might be overrepresented or underrepresented.
What skills are needed to become a data scientist?,"To become a data scientist, one typically needs skills in programming (e.g., Python, R), statistical analysis, machine learning, data wrangling, domain-specific knowledge, communication, and visualization tools, among others."
What is dropout in data science?,"Dropout is a regularization technique used in neural networks where randomly selected neurons are ignored during training, preventing co-adaptation of neurons and helping the model generalize better to unseen data."
What are some deep learning frameworks?,"Some popular deep learning frameworks include TensorFlow, PyTorch, Keras, Caffe, and Theano. These frameworks provide tools to design, train, and validate deep neural networks."
What is an ROC curve?,The ROC (Receiver Operating Characteristic) curve is a graphical plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold varies. It plots the True Positive Rate against the False Positive Rate. The area under the ROC curve (AUC) measures the classifier's performance.
What is a random forest model?,"A random forest is an ensemble machine learning model that consists of multiple decision trees. It aggregates the outputs of individual trees to produce a final result, typically aiming to reduce overfitting and improve generalization."
How is data modeling different from database design?,"Data modeling refers to defining the structure, relationships, and constraints of data you are working with, often visualized as an ERD (Entity Relationship Diagram). Database design, on the other hand, involves the physical implementation of the data model into a database system, considering aspects like performance, scalability, and resilience."
What is accuracy?,Accuracy is a metric that measures the ratio of correctly predicted instances to the total number of instances. It's given by (True Positives + True Negatives) / Total Instances.
What is precision?,Precision measures the ratio of correctly predicted positive instances to the total predicted positives. It's given by True Positives / (True Positives + False Positives).
What is recall?,Recall (or Sensitivity) measures the ratio of correctly predicted positive instances to all actual positives. It's given by True Positives / (True Positives + False Negatives).
What is F1 score?,"The F1 score is the harmonic mean of precision and recall, offering a balance between the two. It's given by 2 * (Precision * Recall) / (Precision + Recall)."
What is a p-value and what is it used for?,"A p-value is a measure used in hypothesis testing to determine the strength of the evidence against the null hypothesis. A small p-value (typically ≤ 0.05) indicates strong evidence against the null hypothesis, suggesting it can be rejected."
What is a t-test and what is it used for?,A t-test is a statistical test used to determine whether there is a significant difference between the means of two groups. It's commonly used when the data sets are approximately normally distributed and have similar variances.
What is a z-score and what is it used for?,"A z-score indicates how many standard deviations an element is from the mean of a dataset. It's used for standardizing scores, identifying outliers, and determining the relative standing of a value within a distribution."
What is the difference between an error and a residual error?,"An error is the difference between the observed value and the true value. A residual, on the other hand, is the difference between the observed value and the predicted value provided by a model. The terms are often used interchangeably, but they have different meanings in a modeling context."
How are machine learning and data science related?,"Machine learning is a subset of data science focused on the development of algorithms that can learn patterns from data. Data science is a broader field that encompasses various aspects of data processing, analysis, visualization, and prediction, using various tools and techniques, including but not limited to machine learning."
Explain univariate analysis.,"Univariate analysis examines one variable at a time. Its purpose is to describe the data and find patterns within it, often visualized using histograms, box plots, or probability distributions."
Explain bivariate analysis.,"Bivariate analysis examines the relationship between two variables to determine if there is an association between them. Techniques include scatter plots, correlation coefficients, and cross-tabulations."
Explain multivariate analysis.,"Multivariate analysis deals with the examination of multiple variables simultaneously. It aims to understand the relationships between them and the effects they have on each other. Common techniques include multiple regression, factor analysis, and multivariate ANOVA."
How can we handle missing data in Python?,"In Python, libraries like Pandas offer methods to handle missing data, including dropping rows or columns (dropna()) or filling in missing values with mean, median, mode, or a specific value (fillna())."
What is a bias-variance tradeoff?,"The bias-variance tradeoff refers to the balance between the error due to bias (underfitting) and the error due to variance (overfitting). Ideally, a model should have low bias and low variance, but improving one often comes at the expense of the other."
What is a prediction model?,A prediction model is an algorithm or statistical model used to determine future outcomes based on input data. It's trained on historical data and then used to predict future events or classify new data points.
What is RMSE?,RMSE (Root Mean Squared Error) is a metric that measures the differences between predicted and observed values. It gives an idea of the magnitude of error and is particularly useful when larger errors are more significant.
What is SVM?,SVM (Support Vector Machine) is a supervised machine learning algorithm used for classification or regression. It aims to find the optimal hyperplane that best separates data classes.
What is a kernel function?,"A kernel function is used in SVM to transform non-linearly separable data into a higher dimension where it becomes linearly separable. Common kernels include linear, polynomial, radial basis function (RBF), and sigmoid."
How do we select an appropriate value of k in k-means?,"Selecting the appropriate k in k-means can be done using methods like the ""elbow method,"" where the variance explained as a function of the number of clusters is plotted, and the ""elbow"" point (where the rate of decrease sharply changes) is a good candidate for k."
What is ensemble learning?,"Ensemble learning involves combining multiple machine learning models to achieve better predictive performance than any of the individual models alone. Techniques include bagging, boosting, and stacking."
What is a/b testing?,"A/B testing, also known as split testing, is an experimental method to compare two versions (A and B) of a webpage, app, or other product against each other to determine which one performs better in terms of a specific metric, such as conversion rate."
What is bagging?,"Bagging, or Bootstrap Aggregating, is an ensemble method that involves creating multiple subsets of the original dataset (with replacement), training a model on each, and then averaging the results (for regression) or voting (for classification) to produce a final prediction."
What is boosting?,"Boosting is an ensemble method where models are trained sequentially, with each new model attempting to correct the errors of the previous ones. This results in a combined model with improved performance."
What is stacking in data science?,"Stacking involves using predictions from multiple models as input for a final ""meta-model"" that makes the final prediction. It leverages the strengths of various models to improve overall accuracy."
How is machine learning different from deep learning?,"Machine learning is a broader field encompassing algorithms that can learn from data. Deep learning is a subset of machine learning focused on algorithms inspired by the structure of the human brain, called artificial neural networks, and particularly on networks with many layers (deep neural networks)."
What does the word Naive mean in Naive Bayes?,"In Naive Bayes, ""Naive"" refers to the assumption that all features are independent of each other given the class label, which is a simplification and often not strictly true in real-world data."
What is batch normalization?,"Batch normalization is a technique used in deep learning to normalize the activations of a layer so that they have a mean close to 0 and a standard deviation close to 1. It can help improve the speed, performance, and stability of neural networks."
"What is the difference between batch, stochastic and mini batch gradient descent?","Batch gradient descent computes the gradient using the entire dataset and updates weights once per epoch. Stochastic gradient descent updates weights after each training example. Mini batch gradient descent is a compromise between the two, updating weights after a subset (mini-batch) of the training data."
What is an activation function?,"An activation function is a function used in neural networks that introduces non-linearity into the model. Common examples include ReLU, sigmoid, and tanh."
How do you build a random forest model?,A random forest model is built by constructing multiple decision trees during training and outputting the mode of the classes for classification or mean prediction of the individual trees for regression. It leverages bagging and feature randomness to ensure diversity among the trees.
How do you avoid overfitting?,"Overfitting can be avoided through techniques like cross-validation, regularization, pruning (for decision trees), increasing training data, using simpler models, and using ensemble methods."
What is variance?,Variance measures the dispersion or spread of a set of data points. It calculates the average of the squared differences from the mean. High variance indicates that the data points are spread out from the mean and from each other.
What is pruning in a decision tree algorithm?,"Pruning involves removing parts of a decision tree that do not provide power to classify instances. It helps reduce the complexity of the final classifier, making it simpler and less prone to overfitting."
What is entropy in a decision tree algorithm?,"Entropy is a measure of the randomness or unpredictability in a data set. In decision trees, it's used to measure the impurity or disorder of a set of instances. The goal in building a decision tree is to have leaves with entropy of 0, indicating pure nodes."
What is time series data?,"Time series data is a sequence of data points, typically consisting of successive measurements made over a time interval. Examples include stock prices over time or daily temperature readings."
What is time series forecasting?,"Time series forecasting involves using past data to predict future data points in the series. Common methods include ARIMA, Exponential Smoothing, and LSTM networks."
What is TF-IDF?,TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). It considers both the frequency of a word in a document and its inverse frequency in the corpus.
How do you deal with unbalanced binary classification?,"Dealing with unbalanced classification can involve resampling techniques (oversampling the minority class or undersampling the majority class), using different evaluation metrics (like F1-score or AUC-ROC), or using algorithms that allow for class weight adjustments."
Which cross-validation method would you use for a batch of time series data?,"For time series data, ""Time Series Split"" or ""Rolling Forecast Origin"" cross-validation is appropriate. It involves training on a continuously expanding window of the data and testing on a fixed-size subsequent window, ensuring data in the past is used to predict data in the future."
What is the difference between point estimates and confidence interval?,"A point estimate provides a single value as an estimate of a parameter (e.g., sample mean). A confidence interval, on the other hand, provides a range of values that likely contains the population parameter with a specified level of confidence."
What is KPI?,KPI stands for Key Performance Indicator. It's a measurable value that demonstrates how effectively an organization is achieving its key business objectives. KPIs can be used at various levels to evaluate success in reaching targets.
What is model fitting?,"Model fitting refers to the process of training a model on data, adjusting its parameters to minimize the difference between the predicted values and the observed values. A well-fitted model will capture the underlying patterns in the data without overfitting or underfitting."
What is robustness in data science?,"Robustness refers to a model's ability to perform well not only on the training data but also on new, unseen data, especially when the data contains noise, outliers, or other unexpected elements. A robust model is resistant to perturbations in the input data."
What is DOE?,DOE stands for Design of Experiments. It's a systematic method to determine the relationship between factors affecting a process and the output of that process. It's used to both optimize and validate manufacturing processes and designs.
What are large language models?,"Large language models (LLMs) are machine learning models, usually based on transformer architectures, trained on vast amounts of text data. They are designed to understand and generate human-like text based on the patterns they've learned from the training data."
What is a transformer in machine learning?,"A transformer is a deep learning model introduced in the ""Attention is All You Need"" paper. It uses self-attention mechanisms to weigh input data differently and can process input elements in parallel (as opposed to sequences), making it highly effective for tasks like machine translation and text generation."
Why is data important in data science?,"Data is the foundation of data science. It provides the raw information that algorithms analyze and learn from. Without data, there would be no insights to draw, no patterns to recognize, and no predictions to make. The quality and quantity of data directly affect the outcomes and reliability of data science projects."
What is R2?,"R^2, or the coefficient of determination, measures the proportion of the variance in the dependent variable that is predictable from the independent variables in a regression model. It ranges from 0 to 1, with 1 indicating perfect predictability."
Are there some other metrics that are better than R2?,"While R^2 is useful, it may not be suitable for all situations. Other metrics, like Adjusted R^2 (which accounts for the number of predictors), RMSE (Root Mean Squared Error), or MAE (Mean Absolute Error) might be more informative in specific contexts or for certain types of models."
What is the curse of dimensionality?,"The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings. It can lead to overfitting and makes clustering, visualization, and optimization challenging."
Is more data always better?,"Not necessarily. While having more data can help improve model performance, it's essential that the data is relevant, clean, and of high quality. Sometimes, more data can introduce noise or redundancy, or increase computational costs without significant benefits."
How do you deal with missing predictors?,"Missing predictors can be handled by various methods such as imputation (filling in missing values using methods like mean, median, or mode), using algorithms robust to missing data, or using techniques like model-based imputation or multiple imputation."
How do you use GPS data to determine the quality of a driver?,"GPS data can be analyzed to derive metrics like speed, acceleration, braking patterns, route efficiency, and adherence to traffic rules. By analyzing these patterns, one can determine driving behaviors like aggressive driving, frequent hard braking, or speeding, which can be used to assess the quality of a driver."
What is regularization in machine learning?,"Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function, thereby constraining the magnitude of model coefficients. Common methods include L1 (Lasso) and L2 (Ridge) regularization."
Why is regularization important?,"Regularization is important because it helps prevent overfitting, ensuring the model generalizes well to new, unseen data. By adding a penalty, it discourages overly complex models which can fit the noise in the training data."
Differentiate between classification and regression algorithms?,"Classification algorithms predict discrete output labels or categories (e.g., spam or not spam). Regression algorithms predict continuous numeric values (e.g., house prices or stock values)."
What is the vanishing gradient problem in deep learning?,"The vanishing gradient problem occurs when gradients of the loss function become too small for earlier layers in a deep network, leading to negligible weight updates and prolonged, ineffective training. It's often observed in networks using activation functions like the sigmoid or tanh."
What is transfer learning?,"Transfer learning is a technique where a pre-trained model on a large dataset is fine-tuned for a different, often smaller, related dataset. It leverages knowledge gained from the initial training task to boost performance on a new task."
What is tokenization in NLP?,"Tokenization is the process of breaking down text into smaller pieces, typically words or subwords, called tokens. It's a crucial preprocessing step in many NLP tasks."
What is the difference between stemming and lemmatization?,"Stemming reduces words to their base or root form (e.g., ""running"" to ""run""), often using heuristic methods. Lemmatization returns the base or dictionary form of a word, considering its morphological analysis (e.g., ""went"" to ""go"")."
What is the bag-of-words model?,"The bag-of-words model represents text as an unordered set of its words, disregarding grammar and word order but maintaining multiplicity. It's a way to convert text into numerical vectors for analysis."
How does sentiment analysis work?,"Sentiment analysis involves determining the emotional tone or attitude expressed in a piece of text. It typically classifies text as positive, negative, or neutral using machine learning models trained on labeled sentiment data. Advanced versions may identify specific emotions or intensities."
What is image segmentation?,"Image segmentation is the process of dividing an image into multiple segments or ""regions,"" where each segment corresponds to different objects or parts of objects. It aims to simplify or change the representation of an image to make it more meaningful or easier to analyze."
Explain the concept of convolution in convolutional neural networks.,"Convolution is a mathematical operation that involves taking a small, trainable filter (or kernel) and sliding it over the input data (like an image) to produce a feature map. In CNNs, this helps in capturing local features in the input, allowing the network to learn spatial hierarchies."
What is optical character recognition and what is it used for?,"Optical Character Recognition (OCR) is the process of converting images of handwritten, typed, or printed text into machine-encoded text. It's used for digitizing printed documents, recognizing text from images, automating data entry, and more."
What is artificial intelligence?,"Artificial Intelligence (AI) is the field of study in computer science that focuses on creating systems capable of performing tasks that require human intelligence, such as visual perception, speech recognition, decision-making, and language translation."
What are the subfields of artificial intelligence?,"The subfields of AI include machine learning, robotics, natural language processing, computer vision, speech processing, expert systems, and more."
What is the Turing test and why is it important in AI?,"The Turing Test, proposed by Alan Turing, is a measure of a machine's ability to exhibit intelligent behavior indistinguishable from a human's. If a machine passes this test, it's considered to have achieved a form of human-like intelligence. It's important as a foundational concept in the philosophy of AI."
What are Type I and Type II errors?,Type I error (false positive) is rejecting a true null hypothesis. Type II error (false negative) is failing to reject a false null hypothesis.
What is hypothesis testing?,Hypothesis testing is a statistical method used to make inferences or decisions about population parameters based on sample data. It involves testing an assumption (null hypothesis) to determine its validity.
What is bootstrapping in statistics?,Bootstrapping is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement. It can be used to assess the robustness of statistical estimates or to derive confidence intervals.
What is the difference between correlation and causation?,"Correlation indicates a relationship between two variables, where changes in one might be associated with changes in another. Causation indicates that a change in one variable directly causes a change in another. Correlation does not imply causation."
What is Bayes theorem?,Bayes' theorem describes the probability of an event based on prior knowledge. It relates the conditional and marginal probabilities of two random events and provides a way to update probabilities with new data.
What are Generative Adversial Networks?,"Generative Adversarial Networks (GANs) are a class of machine learning models where two networks, a generator and a discriminator, are trained together. The generator tries to produce fake data while the discriminator tries to distinguish it from real data. The process refines both networks."
Explain the concept of transfer learning in deep learning,"Transfer learning is a technique where a model trained on one task is repurposed and fine-tuned for a different but related task. It leverages the knowledge gained during the initial training to improve performance on the new task, especially when data for the new task is limited."
"What is an autoencoder, and how is it used in unsupervised learning?","An autoencoder is a type of neural network used to encode data into a lower-dimensional form and then decode it back. It's used for dimensionality reduction, anomaly detection, and feature learning in unsupervised settings."
What is the difference between Word2Vec and GloVe in word embedding models?,"Both Word2Vec and GloVe are methods to create word embeddings. Word2Vec captures semantic meaning based on the local context of words in texts. GloVe captures meaning based on statistical information, specifically on word co-occurrence statistics across a corpus."
Explain the concept of named entity recognition (NER) in NLP.,"Named Entity Recognition (NER) is the process of identifying and classifying named entities (such as persons, organizations, dates, and locations) in text into predefined categories."
What is the purpose of the BLEU metric in machine translation evaluation?,"BLEU (Bilingual Evaluation Understudy) metric evaluates the quality of translated text by comparing it to reference translations. It measures how many n-grams in the machine-generated translations match those in the reference text, providing a score between 0 and 1 (higher is better)."
"How does the attention mechanism work in deep learning, and where is it used in NLP?","The attention mechanism allows models to focus on specific parts of the input when producing an output. In NLP, it's used in sequence-to-sequence models, especially in tasks like machine translation, to weigh the importance of different input words for each output word."
"What are word embeddings, and why are they important in NLP?","Word embeddings are dense vector representations of words that capture their semantic meaning. They are important because they allow models to understand word similarities and relationships, and they reduce dimensionality compared to one-hot encoded representations."
Explain the difference between L1 regularization (Lasso) and L2 regularization (Ridge) in linear regression.,"L1 regularization (Lasso) adds a penalty equivalent to the absolute value of the magnitude of coefficients. L2 regularization (Ridge) adds a penalty equivalent to the square of the magnitude of coefficients. Lasso can induce sparsity (make coefficients zero), while Ridge tends to shrink coefficients."
How can ensemble learning improve the performance of machine learning models?,"Ensemble learning combines predictions from multiple models to produce a final output. By leveraging the strengths of each individual model and averaging out their errors, ensemble methods often achieve higher accuracy and robustness than individual models."
What is the difference between named entity recognition (NER) and part-of-speech tagging (POS tagging)?,"NER identifies and categorizes named entities in text. POS tagging identifies the grammatical parts of speech (like nouns, verbs, adjectives) for each word in a sentence."
What is the difference between lasso and ridge regression?,Both are regularization techniques. Lasso uses L1 regularization which can induce sparsity (make some coefficients zero). Ridge uses L2 regularization which tends to shrink coefficients but doesn't usually set them to zero.
What is cross entropy?,"Cross entropy is a loss function used in classification tasks. It measures the dissimilarity between the true data distribution and the predicted distribution, penalizing incorrect probability assignments."
Difference between precision and recall?,Precision is the fraction of true positive predictions among all positive predictions. Recall (or Sensitivity) is the fraction of true positive predictions among all actual positives.
What is AUC?,AUC (Area Under the ROC Curve) measures the entire two-dimensional area underneath the Receiver Operating Characteristic curve. It evaluates a model's ability to differentiate between the positive and negative class.
What is bias variance trade-off?,"Bias-variance trade-off refers to the balance between the error due to bias (underfitting) and the error due to variance (overfitting). Ideally, a model should have low bias and low variance, but increasing one can decrease the other."
How is correlation related to covariance?,"Both correlation and covariance measure the relationship between two variables. While covariance only indicates the direction of the relationship (positive or negative), correlation additionally provides the degree of the relationship, normalized between -1 and 1."
What are the different types of activation function?,"Common activation functions include Sigmoid, Tanh, ReLU (Rectified Linear Unit), Leaky ReLU, and Softmax, each with its own properties and use-cases."
How can you evaluate classification models?,"Classification models can be evaluated using metrics like accuracy, precision, recall, F1-score, ROC curve, AUC, confusion matrix, among others."
What is clustering?,"Clustering is an unsupervised learning technique that groups similar data points together based on certain features, without having prior labels."
What are the different types of clustering?,"Common types include K-means, Hierarchical clustering, DBSCAN, Gaussian Mixture Models, and Agglomerative clustering."
Can you automate deciding the k in k-means clustering?,"Yes, methods like the Elbow method, Silhouette analysis, and the Gap statistic can help determine the optimal number of clusters (k) in k-means."
How do you treat seasonality in time series?,"Seasonality can be treated using methods like decomposition (which breaks down time series into trend, seasonal, and residual components), seasonal differencing, and using models like SARIMA which account for seasonality."
What to do if your time series data is not stationary?,"Non-stationary time series data can be transformed to be stationary using differencing, taking the logarithm, or using transformations like the Box-Cox transformation."
Difference between ARIMA and SARIMA?,ARIMA (AutoRegressive Integrated Moving Average) models time series data without considering seasonality. SARIMA (Seasonal ARIMA) extends ARIMA to account for seasonality by adding seasonal differencing and seasonal auto-regressive and moving average terms.
What is auto correlation?,Autocorrelation measures the correlation of a time series with its own past and future values. It helps in identifying repeating patterns or seasonality in time series data.
Can we use deep learning for time series forecasting? If yes what models to use? How do they work?,"Yes, deep learning can be used for time series forecasting. Models like LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Units) are popular as they can capture long-term dependencies. They work by utilizing a chain of repeating modules of neural networks to process sequential data."
What is null hypothesis?,"The null hypothesis is a statement in statistics that there is no effect or no difference, and it serves as the default or status quo to be tested against."
What are the different types of word embedding?,"Common types include Bag-of-Words, TF-IDF, Word2Vec, GloVe (Global Vectors for Word Representation), and FastText. These methods convert words into numerical vectors capturing semantic meaning."
What are N-grams?,"N-grams are continuous sequences of 'n' items (words, characters, etc.) from a text or speech. For instance, in the sentence ""I love dogs"", the 2-grams (or bigrams) are: ""I love"" and ""love dogs""."
Is named entity recognition a classification or regression model?,"Named Entity Recognition (NER) is a classification task. It categorizes words or phrases in text into predefined entity classes (e.g., person, organization, location)."
How to handle large data size in case of a NER?,"For large datasets in NER: 1) Use distributed computing (e.g., using frameworks like Spark), 2) Train models using mini-batch gradient descent, 3) Use cloud-based platforms that can handle large-scale data, 4) Consider pruning the dataset if feasible, focusing on the most informative samples."
What is transformer architecture?,"The transformer architecture is a deep learning model introduced in the ""Attention is All You Need"" paper. It uses self-attention mechanisms to weigh input data differently and can process data in parallel, making it efficient and effective for tasks like language translation."
How does BERT work?,BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model trained on large amounts of text using a masked language model task. It captures context from both directions (left-to-right and right-to-left) and can be fine-tuned for specific NLP tasks.
What is attention in NLP? Why is it important?,"Attention mechanisms in NLP allow models to focus on specific parts of the input when producing an output. It's important as it enables the model to weigh the relevance of different parts of input data, enhancing the capture of contextual information."
How to create a model similar to BERT manually?,"Creating a model like BERT manually involves: 1) Defining a transformer architecture with self-attention mechanisms, 2) Pre-training it on a large corpus using a masked language model task, 3) Fine-tuning the pre-trained model on specific downstream tasks. Proper dataset and computational resources are crucial."
How are LLMs designed?,"Large Language Models (LLMs) like GPT-3 or BERT are designed using deep neural architectures, often transformer-based. They are trained on vast amounts of text data, enabling them to generate human-like text and understand context."
In which cases LLMs are disadvantagous?,"LLMs can be: 1) Computationally expensive to train and deploy, 2) Biased based on the data they were trained on, 3) Prone to generating incorrect or nonsensical outputs, 4) Hard to interpret and debug due to their size and complexity."
What are benefits of LLM?,"LLMs: 1) Can generate coherent, diverse, and contextually relevant text, 2) Can be fine-tuned for various NLP tasks with fewer data, 3) Have state-of-the-art performance in many tasks, 4) Offer the ability to generalize across a wide range of tasks without task-specific training data."
How does ChatGPT (Any Generative AI chatbot) remember previous text messages?,"ChatGPT doesn't have long-term memory of past interactions due to privacy concerns. However, during a session, it can consider immediate context provided in the input, allowing for context-aware responses within that session."
"Where should you use open source models like falcon, llama over ChatGPT?","Use open-source models when you need customization, transparency in the model, specific training on niche data, avoiding usage costs associated with proprietary models, or when abiding by certain licensing requirements."
How do LLMs like chatgpt work in the background?,"LLMs, like ChatGPT, utilize deep neural network architectures (often transformer-based). They are pretrained on vast amounts of text and use patterns learned to generate or classify text based on the input they receive."
How can you fine tune LLMs?,"Fine-tuning involves training the pre-trained LLM on a specific, smaller dataset related to a particular task, allowing the model to adapt to specific nuances and requirements of that task."
How does ChatGPT generate text responses?,"ChatGPT uses its learned patterns from training data to predict the next word in a sequence, generating text word by word until it forms a coherent response or reaches a specified length."
How do you design conversations in a chatbot?,"Designing conversations involves understanding user intents, defining possible user inputs, crafting appropriate bot responses, handling fallbacks for unrecognized inputs, and maintaining context for coherent multi-turn conversations."
How does dialogflow or rasa work in designing chatbots?,"Dialogflow and Rasa use intent recognition and entity extraction to understand user input. Based on the recognized intent and entities, they generate or fetch appropriate responses. Both platforms allow developers to define conversation flows and handle various user intents."
How to evaluate a chatbot?,"Evaluation can be based on accuracy of intent recognition, quality of responses, user satisfaction surveys, the chatbot's ability to handle unexpected inputs, and efficiency in task completion."
What metrics do you use to monitor a chatbot?,"Common metrics include: Response time, user retention rate, session length, user satisfaction scores, fallback rate (unrecognized inputs), and task success rate."
What is topic modeling?,Topic modeling is an unsupervised machine learning method used to identify topics in a set of documents. Algorithms like Latent Dirichlet Allocation (LDA) are commonly used for this.
Explain xgboost.,"XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It boosts tree-based models' performance and accuracy through regularization and handling of missing data."
What is an Adam optimizer?,Adam (Adaptive Moment Estimation) is a gradient descent-based optimization algorithm that computes adaptive learning rates for each parameter. It combines the benefits of two other extensions of gradient descent: AdaGrad and RMSProp.
What is PCA?,"PCA (Principal Component Analysis) is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional form, capturing the most variance in the data using fewer components."
What is back propagation?,"Back propagation is an optimization algorithm used for minimizing the error in neural networks. It calculates the gradient of the error function concerning each weight by applying the chain rule, then updates the weights to minimize the error."
What is conditional probability?,Conditional probability is the probability of an event occurring given that another event has already occurred. It's represented as P(A
What is exponential distribution?,The exponential distribution describes the time intervals in a Poisson point process or the time between consecutive events in a continuous-time process with a constant event rate.
What is binomial distribution?,"The binomial distribution describes the number of successes in a fixed number of independent Bernoulli trials, with the same probability of success on each trial."
What is Bernouilli distribution?,"The Bernoulli distribution is a discrete probability distribution for a random variable which can take only two outcomes, typically labeled 0 and 1 (e.g., success/failure)."
What is chi-squared test?,The chi-squared test assesses whether observed frequencies are different from expected frequencies in categorical variables. It's used for independence testing between categorical variables.
What is gamma distribution?,"The gamma distribution is a two-parameter family of continuous probability distributions, which is widely used in statistics to model continuous variables that are always positive and have skewed distributions."
What is the Central Limit Theorem?,"The Central Limit Theorem states that the distribution of the sum (or average) of a large number of independent, identically distributed random variables approaches a Gaussian (normal) distribution, regardless of the original distribution of the variables."
What is marginal probability?,Marginal probability is the probability of an event occurring without reference to any other events; it's derived by summing joint probabilities over all values of the other variables.
Explain collinearity.,"Collinearity refers to a condition where two or more predictor variables in a regression are highly correlated, meaning one can be linearly predicted from the others, which can distort the coefficient estimates and reduce the model's interpretability."
What metrics are used to classify datasets?,"Classification metrics include accuracy, precision, recall, F1 score, ROC-AUC, confusion matrix, and more, which evaluate the performance of classification algorithms."
What is co-efficient in statistics?,"In statistics, a coefficient typically refers to the numerical or constant quantity placed before and multiplying a particular variable in an equation (e.g., the slope in a linear regression equation). It indicates the degree or magnitude of a certain relationship or effect."
"What are mean, median and mode?",Mean is the average of all values in a dataset. Median is the middle value in a sorted dataset. Mode is the value that appears most frequently in a dataset.
What is descriptive statistics?,"Descriptive statistics summarizes and describes the main features of a dataset through measures such as mean, median, mode, standard deviation, etc., providing a simple overview of the sample and measures of the data's main characteristics."
What is inferential statistics?,Inferential statistics uses a random sample of data from a population to describe and make inferences about the population. It often involves hypothesis testing and the estimation of population parameters.
What is the difference between predictive and prescriptive analytics?,"Predictive analytics forecasts future trends using historical data while prescriptive analytics suggests actions to take based on predictions, aiming at achieving specific outcomes or solving particular problems."
What is the learning curve tool?,A learning curve plots the performance of a model (like accuracy or error) against the experience or training time of the model. It helps in understanding if a model benefits from more data or if it's overfitting/underfitting.
What is contour visualization?,"Contour visualization represents a 3-dimensional surface on a 2-dimensional plane using contour lines, which are lines of constant value. It's commonly used to visualize the relationship between three continuous variables."
Give different types of optimization algorithms.,"Some optimization algorithms include Gradient Descent, Stochastic Gradient Descent, Newton's Method, Genetic Algorithms, and Simulated Annealing."
What is a decision boundary?,A decision boundary is a surface or line that separates data points belonging to different classes in a classification problem. It represents the threshold where the probability of belonging to one class vs. another is equal.
What is a kernel trick?,"The kernel trick involves using a kernel function to transform data into a higher-dimensional space, making it possible to perform linear separation in that space for problems that aren't linearly separable in the original space."
How do you tune parameters to avoid underfitting and overfitting?,"Use techniques like cross-validation to evaluate model performance on unseen data, regularization to penalize complex models, pruning in decision trees, and gathering more data if possible."
How do we split data into training and testing data?,"Data can be split randomly, but often it's divided such that around 70-80% is used for training and 20-30% is used for testing to validate the model's performance. Tools like train_test_split in scikit-learn can help."
Explain TPE hyperparameter optimization.,TPE (Tree-structured Parzen Estimator) is a Bayesian optimization method. It builds a probabilistic model of the objective function and selects new inputs to evaluate the function by applying a criterion to the model.
Explain the memory cell of a LSTM.,"The memory cell in an LSTM (Long Short-Term Memory) network is its core component, regulating the flow of information through controlling gates. It can remember or forget specific information over long time intervals, making LSTMs effective for sequence prediction tasks."
"What is the difference between loss function, cost function and objective function?",A loss function computes the error for a single training example. Cost function is the average loss over the entire dataset. Objective function is a more general term that may include the cost function and regularization terms.
What does it mean when p values are high or low?,A low p-value (< 0.05) suggests that the observed effect is statistically significant and not due to random chance. A high p-value suggests the opposite.
What are the differences between expected and mean value?,"They are often used interchangeably in many contexts. The expected value is the long-run average value of a random variable, whereas the mean is the average value of a dataset."
What are confounding variables?,"Confounding variables are external factors in an experiment or study that affect both the independent and dependent variables, potentially leading to incorrect conclusions."
What is k-nearest neighbor algorithm?,"KNN is a non-parametric, instance-based learning algorithm where the output of a new instance is predicted based on the k closest training examples in the feature space."
"What are true positives, false positives, true negatives and false negatives?",True positives (TP) are correctly predicted positives. False positives (FP) are incorrectly predicted positives. True negatives (TN) are correctly predicted negatives. False negatives (FN) are incorrectly predicted negatives.
What is multicollinearity?,"Multicollinearity occurs when independent variables in a regression model are highly correlated, which can destabilize the model and make it hard to identify the effect of individual variables."
When would you use random forests vs. SVM and why?,Random forests are preferred for large datasets and when interpretability and feature importance are crucial. SVMs are powerful for higher-dimensional data and when maximum margin separation is desired.
Do you think 50 small decision trees are better than a large one? Why?,"It depends. 50 small trees, like in a random forest, can capture complex patterns and reduce overfitting. A large tree might fit the training data very well but overfit and perform poorly on unseen data."
What’s the difference between an AdaBoosted tree and a Gradient Boosted tree?,"AdaBoost adjusts the weights of misclassified instances at each iteration, whereas Gradient Boosting adjusts based on the negative gradient (or residuals) of the loss function."
How does XGBoost handle the bias-variance tradeoff?,"XGBoost uses gradient boosting algorithm with regularization terms, which controls the complexity of trees, helping in managing the bias-variance tradeoff."
What is the difference between online and batch learning?,"Online learning updates the model incrementally with each new data point, whereas batch learning uses the entire dataset to train the model at once."
Is mean imputation of missing data acceptable practice?,"While it's a simple method, mean imputation doesn't account for inherent variability in data and can reduce the variance. It's generally better to use more sophisticated methods or understand the nature of missingness first."
What is an inlier?,An inlier is a data point that falls within the expected range and is not considered an outlier or anomaly.
How are collaborative filtering and content-based filtering similar?,"Both are recommendation system techniques. Collaborative filtering recommends items based on user behavior, while content-based filtering recommends items similar to what a user likes. The similarity is in their goal: to provide personalized recommendations."
What are some of the steps for data wrangling and data cleaning before applying machine learning algorithms?,"Steps include handling missing values, removing duplicates, transforming variables, dealing with outliers, encoding categorical variables, and normalization or scaling."
What is the difference between precision and recall?,"Precision is the fraction of relevant instances among the retrieved instances, while recall (sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances."
Why is mean square error a bad measure of model performance?,"MSE is sensitive to outliers, so it might give an overly pessimistic measure if there are large errors. It also lacks interpretability in some contexts since it doesn't have the same unit as the original data."
What is feature engineering?,Feature engineering is the process of creating new features or modifying existing ones to improve the performance of machine learning models.
What are the feature selection methods used to select the right variables?,"Methods include filter methods (e.g., correlation coefficients), wrapper methods (e.g., forward selection, backward elimination), and embedded methods (e.g., LASSO, decision trees)."
Briefly explain how a basic neural network works,"A neural network processes input data through layers of interconnected nodes (neurons). Each connection has a weight, and nodes apply an activation function to the incoming data. The network learns by adjusting weights based on the error in predictions."
Why is Rectified Linear Unit a good activation function?,"ReLU is computationally efficient, helps mitigate the vanishing gradient problem, and often leads to faster convergence in practice."
What happens if the learning rate is set too high or too low?,"Too high: the model might overshoot the optimal point, leading to divergence. Too low: the model converges slowly or gets stuck in local minima."
What is the law of large numbers?,"It states that as the size of a sample increases, the sample mean will get closer to the population mean."
Why is Central Limit Theorem important?,"It states that, given certain conditions, the mean of a large number of independent and identically distributed random variables tends toward a normal distribution, regardless of the original distribution. It underpins many statistical methods and tests."
What is the Markov Property?,"It states that the future state of a process is only dependent on the current state, not on the sequence of states that preceded it."
What is statistical power?,The probability that a test correctly rejects the null hypothesis when the alternative hypothesis is true.
How does experimental data contrast with observational data?,"Experimental data is collected from experiments where researchers control one or more variables. Observational data is collected without any intervention from the researcher, observing natural occurrences."
Give an example where the median is a better measure than the mean,"Incomes in a city where a few billionaires skew the mean upwards, making the median a better representation of a typical resident's income."
What is survivorship bias?,"It's the error of focusing on the subjects that ""survived"" some process and overlooking those that didn't, leading to false conclusions."
What is root cause analysis? How can you identify a cause vs. a correlation?,"Root cause analysis is a method of problem-solving aimed at identifying the root causes of problems or events. Identifying cause vs. correlation involves controlled experiments, understanding the underlying mechanisms, and using statistical methods to rule out spurious relations."
Explain what a long-tailed distribution is,"A distribution with a large number of occurrences far from the mode, and few near it. It indicates a high degree of variance or outliers."
When is a/b testing used?,"It's used to compare two versions (A and B) of a webpage, app, or other product to determine which performs better in terms of a specific metric."
How do you assess the statistical significance of an insight?,"By conducting hypothesis tests and calculating p-values. If the p-value is below a predetermined threshold (e.g., 0.05), the result is deemed statistically significant."
What is the difference between a boxplot and a histogram?,"A boxplot provides a summary of a data set's central tendency, variability, and skewness using quartiles, medians, and possible outliers. A histogram shows the frequency distribution of a data set."
What is the meaning of ACF and PACF?,ACF (Auto-Correlation Function) measures the correlation between a series and its lagged values. PACF (Partial Auto-Correlation Function) measures the correlation between a series and its lagged values while controlling for the values at all shorter lags.
Why is dimension reduction important?,"Reduces computational costs, removes multicollinearity, helps in visualizing data, and may improve model performance by removing noise."
Is it beneficial to perform dimensionality reduction before fitting an SVM? Explain.,"Yes, it can speed up training, particularly in cases where there's a lot of redundant or irrelevant features. However, it may lose some information, so it's a trade-off."
Difference between convex and non-convex cost function,"A convex function has a single global minimum, and any local minimum is also a global minimum. Non-convex functions have multiple local minima, making optimization more challenging."
How do you calculate the needed sample size?,"Sample size calculation depends on desired confidence level, margin of error, population variance, and effect size. Common methods include Cochran's formula for simple random samples."
"Is rotation necessary in PCA? If yes, Why?","Rotation in PCA (like Varimax) can help in making the loadings of components more interpretable. It maximizes high loadings and minimizes low loadings, making it clearer which variables drive each principal component."
"Explain prior probability, likelihood and marginal likelihood in context of Naive Bayes.",Prior Probability: Initial belief about the probability of an outcome. Likelihood: Probability of observing the data given a particular outcome. Marginal Likelihood: Total probability of observing the data across all possible outcomes.
How is kNN different from kmeans clustering?,kNN is a supervised classification algorithm that assigns a new data point to the majority class of its 'k' nearest neighbors. kmeans is an unsupervised clustering algorithm that partitions data into 'k' clusters based on feature similarity.
How is True Positive Rate and Recall related?,They are the same. Both measure the proportion of actual positives that are correctly identified.
When is Ridge regression favorable over Lasso regression?,"Ridge is preferable when there are many large parameters of about the same value (i.e., when most predictors are important). Lasso, due to its ability to reduce some coefficients to zero, is preferable when we believe many features are irrelevant or redundant."
What is convex hull?,The convex hull of a set of points is the smallest convex shape that contains all the points.
What is Euclidean and Manhattan distance? What are they used for?,Euclidean Distance: Straight line distance between two points in a space. Manhattan Distance: Sum of absolute differences in their coordinates (like moving only on gridlines of a grid). Both are used in various algorithms to measure the distance/similarity between data points.
What is ordinary least squares?,A method to estimate the parameters in a linear regression model by minimizing the sum of squared differences between observed and predicted values.
What is maximum likelihood?,"A method used to estimate the parameters of a statistical model by maximizing the likelihood function, which measures how well the model explains the observed data."
Explain the steps in making a decision tree.,"1. Choose an attribute to split on. 2. Calculate the best split point (using metrics like Gini impurity or entropy). 3. Create a node based on that attribute. 4. Repeat the process for the sub-nodes using the subsets of data. 5. Stop when a termination condition is met (e.g., max depth reached, minimum samples per leaf, or purity achieved)."
How should you maintain a deployed model?,"Continuously monitor its performance, periodically retrain with new data, adjust for concept drift, ensure infrastructure support, and seek user feedback."
How can time-series data be declared as stationary?,"A time series is stationary if its statistical properties (mean, variance, autocorrelation) don't change over time. Tests like the Augmented Dickey-Fuller test can be used to check for stationarity."
What are the feature vectors?,A feature vector is an n-dimensional vector of numerical features that represent an object in machine learning.
Explain Dependency Parsing in NLP.,"Dependency parsing identifies grammatical relationships between words in a sentence, linking words based on their dependencies. E.g., in ""She read the book"", ""read"" is linked to ""She"" (subject) and ""book"" (object)."
What is text summarization?,"The process of shortening a text document, retaining its most important information, to produce a concise summary."
What is NLTK? How is it different from Spacy?,"NLTK (Natural Language Toolkit) is a Python library for NLP offering a wide range of tools and English grammars. SpaCy is another Python NLP library, designed to be fast and production-ready. While NLTK is broad, SpaCy is optimized for performance and industrial use."
What is SpaCy?,"SpaCy is a high-performance, industrial-strength NLP library in Python designed for production use. It offers pre-trained models and supports various NLP tasks."
What is information extraction in NLP?,"The process of automatically extracting structured information like entities, relationships, and events from unstructured text."
What is Pragmatic Ambiguity in NLP?,Ambiguity arising from context or the way something is said rather than language structure or word meanings. It's about interpreting the speaker's intended meaning.
What is cosine similarity in NLP?,"A measure of similarity between two non-zero vectors, often used to measure similarity between text documents represented as vectors (e.g., TF-IDF vectors)."
What is Masked Language Model?,"A type of language model where some words in the input are masked (hidden), and the model is trained to predict those masked words from their context. BERT is an example."
What is the difference between NLP and CI (Conversational Interface)?,NLP deals with the processing and understanding of human language. CI (or chatbot) uses NLP techniques to facilitate human-computer interaction in a conversational manner.
Explain Dependency Parsing in NLP?,"(Repeated question; see above) Dependency parsing identifies grammatical relationships between words in a sentence, based on their dependencies."
What are the various steps involved in an analytics project?,"Problem definition, data collection, data cleaning, exploratory data analysis, feature engineering, model selection, model training, evaluation, deployment, and monitoring."
What are hyperparameters?,"Parameters not learned from the training data, set before training starts. Examples: learning rate, number of hidden layers in a neural network."
How Does an LSTM Network Work?,"LSTM (Long Short-Term Memory) is a type of recurrent neural network that can learn and remember over long sequences and is less susceptible to the vanishing gradient problem. It uses gates (input, output, forget) to regulate information flow."
What Is a Multi-layer Perceptron (MLP)?,"A type of feedforward artificial neural network consisting of multiple layers of nodes in a directed graph, where each layer is fully connected to the next one. It's used for classification and regression tasks."
What is exploding gradients?,
What are the variants of Back Propagation?,What are the variants of Back Propagation?
What Do You Mean by Tensor in Tensorflow?,
"Explain the 80/20 rule, and tell me about its importance in model validation.",
What is a model in machine learning?,
How would you go about choosing an algorithm to solve a business problem?,
"What’s the difference between inductive, deductive, and abductive learning?",
What steps would you take to evaluate the effectiveness of your ML model?,
What’s a Fourier transform?,
What’s the difference between a generative and discriminative model?,
How would you build a data pipeline?,
Is more data always better?,
How can you make sure that you don’t analyze something that ends up meaningless?,
How can you determine which features are the most important in your model?,
What are some ways I can make my model more robust to outliers?,
"What are some differences you would expect in a model that minimizes squared error, versus a model that minimizes absolute error?",
What error metric would you use to evaluate how good a binary classifier is?,
"Given training data on tweets and their retweets, how would you predict the number of retweets of a given tweet after 7 days after only observing 2 days worth of data?",
"In an A/B test, how can you check if assignment to the various buckets was truly random?",
"What might be the benefits of running an A/A test, where you have two buckets who are exposed to the exact same product?",
What would be the hazards of letting users sneak a peek at the other bucket in an A/B test?,
How would you design an experiment to determine the impact of latency on user engagement?,
What is maximum likelihood estimation? Could there be any case where it doesn’t exist?,
What is a confidence interval and how do you interpret it?,
What is Selection Bias?,
How do we choose K in K-fold cross-validation?,
What is sigmoid? What does it do?,
How to interpret the AU ROC score?,
What is the area under the PR curve? Is it a useful metric?,
In which cases AU PR is better than AU ROC?,
What do we do with categorical variables?,
Can we have both L1 and L2 regularization components in a linear model?,
What’s the interpretation of the bias term in linear models?,
How do we interpret weights in linear models?,
If a weight for one variable is higher than for another  —  can we say that this variable is more important?,
When do we need to perform feature normalization for linear models? When it’s okay not to do it?,"Feature normalization is necessary for L1 and L2 regularizations. The idea of both methods is to penalize all the features relatively equally. This can't be done effectively if every feature is scaled differently. Linear regression without regularization techniques can be used without feature normalization. Also, regularization can help to make the analytical solution more stable, — it adds the regularization matrix to the feature matrix before inverting it."
Which feature selection techniques do you know?,
Can we use L1 regularization for feature selection?,
What are the benefits of a single decision tree compared to more complex models?,
Why do we need randomization in random forest?,
How do we select the depth of the trees in random forest?,
How do we know how many trees we need in random forest?,
What happens when we have correlated features in our data?,
What’s the difference between random forest and gradient boosting?,
Is it possible to parallelize training of a gradient boosting model? How to do it?,
"When using scikit-learn, is it true that we need to scale our feature values when they vary greatly?",
"For NLP, what’s the main purpose of using an encoder-decoder model?",
Why does XGBoost perform better than SVM?,
What is the default method for splitting in decision trees?,
What are advantages and disadvantages of using neural networks?,
You are working on a dataset. How do you select important variables?,
How do you fix high variance in a model?,
Explain LDA for unsupervised learning.,
How do you choose a classifier based on a training set size?,
What are the most common algorithms for supervised learning and unsupervised learning?,
What are the problems with using trees for solving time series problems?,
How is time series different from the usual regression problem?,
Possible approaches to solving the cold start problem?,
What is the cold start problem?,
"How we can incorporate implicit feedback (clicks, etc) into our recommender systems?",
What are good baselines when building a recommender system?,
What’s singular value decomposition? How is it typically used for machine learning?,What is precision and recall at k?
Do you know any dimensionality reduction techniques?,
"If you have a sentence with multiple words, you may need to combine multiple word embeddings into one. How would you do it?","If you have a sentence with multiple words, you may need to combine multiple word embeddings into one. How would you do it?"
Would you prefer gradient boosting trees model or logistic regression when doing text classification with bag of words?,
How is TF-IDF useful for text classification?,
What are the advantages and disadvantages of bag of words?,
How to choose which augmentations to use? (neural networks),How to choose which augmentations to use?
What kind of augmentations do you know?,
Are CNNs resistant to rotations? What happens to the predictions of a CNN if an image is rotated?,
What’s pooling in CNN? Why do we need it?,
How does max pooling work? Are there other pooling techniques?,
Why do we actually need convolutions? Can’t we use fully-connected layers for that?,
What is model checkpointing?,
What’s a convolutional layer?,
How do we decide when to stop training a neural net?,
What is Adam? What’s the main difference between Adam and SGD?,
What is a learning rate and how do we set it?,
Which optimization techniques for training neural nets do you know?,
How we can initialize the weights of a neural network?,
What if we set all the weights of a neural network to 0?,
What regularization techniques for neural nets do you know?,
What is ReLU? How is it better than sigmoid or tanh?,
Which hyper-parameter tuning strategies (in general) do you know?,
What’s the difference between grid search parameter tuning strategy and random search? ,
What are stop words?,
What is Syntactic Analysis?,
What is Semantic Analysis?,
What is Entity extraction?,
What is Latent Semantic Indexing (LSI)?,
What are Regular Expressions?,
What is Regular Grammar?,
What is Parsing in the context of NLP?,
What is POS tagging?,
Explain the process of text classification.,
What is the difference between NLP and NLU?,
"What are unigrams, bigrams, trigrams, and n-grams in NLP?",
What are the steps involved in solving an NLP problem?,
What is feature extraction in NLP?,
How to tokenize a sentence using the nltk package?,
How to check word similarity using the spaCy package,
What is a perceptron?,
 What is the difference between lists and tuples?,
Explain Inheritance in Python with an example?,
What is object-oriented programming?,
What is a dictionary in Python and what is it used for?,
What are the common built-in data types in Python?,
What are local variables and global variables in Python?,
What is the difference between Python Arrays and lists?,
